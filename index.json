[{"content":"What is Hugo? Hugo is a fast and modern static site generator written in Go. It allows developers to create websites quickly by converting Markdown content into static HTML files. Known for its speed and flexibility, Hugo is widely used for blogs, portfolios, and documentation sites. It supports themes, custom layouts, and a wide range of content formats, making it a powerful tool for building static websites.\nKey Features of Hugo Blazing Fast: Hugo is one of the fastest static site generators, capable of building thousands of pages in seconds. Markdown-Based: Write your content in Markdown, and Hugo will handle the rest. Themes: Choose from a wide variety of community-built themes to customize your site. Flexible Content Organization: Hugo allows you to organize your content in any way that suits your project. Built-in Shortcodes: Use shortcodes to add dynamic elements like videos, images, and more. Multilingual Support: Easily create multilingual websites with Hugo\u0026rsquo;s built-in internationalization features. Why Use Hugo? Hugo is ideal for developers and content creators who want a simple, fast, and flexible way to build static websites. Whether you\u0026rsquo;re creating a personal blog, a portfolio, or a documentation site, Hugo provides the tools you need to get started quickly and efficiently.\nGetting Started with Hugo Install Hugo on your system. Create a new site using the hugo new site \u0026lt;sitename\u0026gt; command. Add content in Markdown files under the content directory. Use hugo server to preview your site locally. Build your site with hugo and deploy it to your preferred hosting platform. For more details, visit the official Hugo documentation.\n","permalink":"https://almat101.github.io/blog/hugo/hugo/","summary":"\u003ch2 id=\"what-is-hugo\"\u003eWhat is Hugo?\u003c/h2\u003e\n\u003cp\u003eHugo is a fast and modern static site generator written in Go. It allows developers to create websites quickly by converting Markdown content into static HTML files. Known for its speed and flexibility, Hugo is widely used for blogs, portfolios, and documentation sites. It supports themes, custom layouts, and a wide range of content formats, making it a powerful tool for building static websites.\u003c/p\u003e\n\u003ch3 id=\"key-features-of-hugo\"\u003eKey Features of Hugo\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eBlazing Fast\u003c/strong\u003e: Hugo is one of the fastest static site generators, capable of building thousands of pages in seconds.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMarkdown-Based\u003c/strong\u003e: Write your content in Markdown, and Hugo will handle the rest.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eThemes\u003c/strong\u003e: Choose from a wide variety of community-built themes to customize your site.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eFlexible Content Organization\u003c/strong\u003e: Hugo allows you to organize your content in any way that suits your project.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuilt-in Shortcodes\u003c/strong\u003e: Use shortcodes to add dynamic elements like videos, images, and more.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMultilingual Support\u003c/strong\u003e: Easily create multilingual websites with Hugo\u0026rsquo;s built-in internationalization features.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"why-use-hugo\"\u003eWhy Use Hugo?\u003c/h3\u003e\n\u003cp\u003eHugo is ideal for developers and content creators who want a simple, fast, and flexible way to build static websites. Whether you\u0026rsquo;re creating a personal blog, a portfolio, or a documentation site, Hugo provides the tools you need to get started quickly and efficiently.\u003c/p\u003e","title":"Hugo"},{"content":"ðŸ”— Subject Transcendence Multi-Service Dockerized Application This project is about creating a multi-service application orchestrated with Docker Compose. It integrates static frontend, backend microservices, advanced monitoring tools, and an ELK stack for centralized logging and analytics. The architecture emphasizes modularity, scalability, and seamless deployment, making it robust and efficient for modern application needs.\nTable of Contents Overview Services Proxy Backend Services Monitoring ELK Stack Setup Usage Volumes Networks Overview This project is a Single Page Application (SPA) that replicates the classic Pong game with modern features and a robust backend architecture. It includes:\nSecure Login: User authentication and authorization for a personalized experience. Tournament Management: Organize and participate in tournaments with other players. AI Opponent: Play against an AI-powered opponent for solo gameplay. Match History: Track and review the history of all matches played. Monitoring and Analytics: Tools like Grafana, Prometheus, and Alertmanager for real-time monitoring and alerting. Centralized Logging: An ELK stack (Elasticsearch, Logstash, Kibana) for managing and visualizing logs. Reverse Proxy: An nginx proxy server for secure routing and load balancing. The application is containerized using Docker and orchestrated with Docker Compose, ensuring scalability, modularity, and ease of deployment.\nServices Proxy The Proxy service acts as a reverse proxy and includes an API Gateway implemented within Nginx. It provides the following features:\nImage: proxy Ports: 8090:8090 Healthcheck: Ensures the proxy is running and accessible. Dependencies: Relies on backend services (e.g., authentication, tournament, history), monitoring tools (e.g., Grafana, Prometheus), and the ELK stack. The Nginx configuration includes API Gateway functionality, making it a lightweight yet effective API Gateway. It handles:\nRouting: Directs API requests to appropriate backend services based on the URL path (/api/user/ to auth-service, /api/tournament/ to tournament-service and /api/history/ to history-service). Rate Limiting: Implements request rate limiting using the api_limit zone to prevent abuse. Authentication: Validates JWT tokens for protected routes and extracts user information for authenticated requests. Security: Adds standard proxy headers and enforces HTTPS with strong SSL/TLS configurations. Static File Serving: Serves the frontend static files, which are copied into the container during the Docker build process, ensuring efficient delivery of the application\u0026rsquo;s static assets. Backend Services Auth Service\nDescription: Handles user authentication and authorization, including secure login and OAuth integration. Framework: Built with Django. Health Check: Exposes health checks at /watchman/?skip=watchman.checks.storage. History Service\nDescription: Manages the storage and retrieval of match history, allowing users to review past games. Framework: Built with Django. Health Check: Exposes health checks at /watchman/. Tournament Service\nDescription: Provides functionality for creating, managing, and participating in tournaments. Framework: Built with Django. Health Check: Exposes health checks at /watchman/. Databases\nDescription: Each backend service is paired with its own PostgreSQL database to ensure data isolation and scalability. Services: auth_db: Stores user authentication and authorization data. history_db: Stores match history data. tournament_db: Stores tournament-related data. Monitoring Grafana\nDescription: A powerful visualization and monitoring dashboard that provides insights into system and application performance. Features: Two pre-configured dashboards: Django Dashboard: Allows switching between all microservices (e.g., auth-service, history-service, tournament-service) to monitor metrics like request rates, response times, and errors. Node Exporter Dashboard: Displays detailed system metrics, including CPU usage, RAM utilization, disk space, and network activity. Access: Available at https://localhost/grafana/. Prometheus\nDescription: A metrics collection and monitoring system that scrapes data from various services and evaluates alerting rules. Features: Collects metrics from all microservices, PostgreSQL databases, and the Nginx proxy. Evaluates alerting rules defined in rules_1.yml, rules_2.yml, rules_3.yml, and rules_4.yml. Healthcheck: Endpoint available at http://localhost:9090/-/healthy. Node Exporter\nDescription: A lightweight exporter that collects system-level metrics from the host machine. Features: Provides data on CPU usage, memory availability, disk space, and network performance. Integrated with the Node Exporter Dashboard in Grafana for detailed visualization. Alertmanager\nDescription: Manages alerts generated by Prometheus and routes them to configured notification channels. Features: Sends alerts via email for critical issues, such as service downtime, high CPU usage, low disk space, or memory exhaustion. Configured with alerting rules for all microservices and system metrics. Postgres Exporters\nDescription: Specialized exporters that collect metrics from PostgreSQL databases. Features: Monitors database health, active connections, query performance, replication lag, and disk usage. Metrics are visualized in Grafana and used for alerting in Prometheus. Alerts and Notifications Alert Rules: Defined in rules_1.yml, rules_2.yml, rules_3.yml, and rules_4.yml to monitor critical conditions across all services and infrastructure. Examples: Instance Down: Alerts when any service or exporter is unreachable. High CPU Load: Triggers when CPU usage exceeds 80% for more than 5 minutes. Low Disk Space: Alerts when disk space falls below 10%. PostgreSQL Issues: Monitors database downtime, high connection counts, slow queries, and replication lag. Nginx Proxy Down: Alerts if the Nginx proxy becomes unreachable. Notification: Alerts are sent via email to ensure timely responses to critical issues. ELK Stack Elasticsearch\nDescription: A distributed search and analytics engine that serves as the centralized storage for logs and analytics. Features: Stores and indexes logs from various sources, including Nginx access.log. Provides powerful search and aggregation capabilities for log analysis. Security: Fully secured with SSL and authentication. Certificates for Elasticsearch are generated by a dedicated setup container, which creates all necessary certificates for Elasticsearch and Kibana. Configured to use HTTPS for all communications, ensuring data integrity and privacy. Integration: Works seamlessly with Logstash for log ingestion and Kibana for visualization. Logstash\nDescription: A data processing pipeline that ingests, transforms, and forwards logs to Elasticsearch. Features: Parses Nginx access.log files and enriches the data for analysis. Configured to securely communicate with Elasticsearch using SSL certificates generated by the setup container. Role in the Stack: Acts as the intermediary between Nginx logs and Elasticsearch, ensuring logs are properly formatted and indexed for analysis. Kibana\nDescription: A visualization and exploration tool for Elasticsearch data. Features: Provides a pre-configured dashboard to analyze Nginx access.log data, including request rates, response statuses, client IPs, and more. Allows users to query and visualize Elasticsearch data with ease. Access: Available at https://localhost/kibana. Security: Uses SSL for secure communication with Elasticsearch. Certificates for secure communication are generated by the setup container. Dashboards: Includes a dedicated dashboard for analyzing all aspects of the Nginx access.log, providing insights into traffic patterns, error rates, and client activity. Additional Notes Setup Container:\nA dedicated container (setup) is responsible for generating all SSL certificates required by Elasticsearch and Kibana. Ensures that all components of the ELK stack communicate securely using HTTPS. Nginx Access Logs:\nThe ELK stack is configured to parse and analyze Nginx access.log files. Kibana\u0026rsquo;s dashboard provides detailed insights into Nginx traffic, including request patterns, error rates, and client activity. Secure Communication:\nAll components of the ELK stack (Elasticsearch, Logstash, and Kibana) are configured to use SSL for secure communication, ensuring data privacy and integrity. ","permalink":"https://almat101.github.io/projects/transcendence/transcendence/","summary":"\u003ch3 id=\"-subject\"\u003eðŸ”— \u003ca href=\"\"\u003eSubject\u003c/a\u003e\u003c/h3\u003e\n\u003ch1 id=\"transcendence\"\u003eTranscendence\u003c/h1\u003e\n\u003ch1 id=\"multi-service-dockerized-application\"\u003eMulti-Service Dockerized Application\u003c/h1\u003e\n\u003cp\u003eThis project is about creating a multi-service application orchestrated with Docker Compose. It integrates static frontend, backend microservices, advanced monitoring tools, and an ELK stack for centralized logging and analytics. The architecture emphasizes modularity, scalability, and seamless deployment, making it robust and efficient for modern application needs.\u003c/p\u003e\n\u003ch2 id=\"table-of-contents\"\u003eTable of Contents\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#overview\"\u003eOverview\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#services\"\u003eServices\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#proxy\"\u003eProxy\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#backend-services\"\u003eBackend Services\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#monitoring\"\u003eMonitoring\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#elk-stack\"\u003eELK Stack\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#setup\"\u003eSetup\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#usage\"\u003eUsage\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#volumes\"\u003eVolumes\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#networks\"\u003eNetworks\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cp\u003eThis project is a Single Page Application (SPA) that replicates the classic Pong game with modern features and a robust backend architecture. It includes:\u003c/p\u003e","title":"Transcendence"},{"content":"ðŸ”— Subject Description In this project, done togheter with my peer Manuele Longo we developed a fully functional HTTP web server built in C++98, supporting essential web server features like GET, POST, and DELETE requests, CGI handling, and more.\nPurpose The purpose of this project is to create a robust and efficient web server that can handle various HTTP requests, serve static and dynamic content, and manage multiple virtual hosts. The server is designed to be lightweight, performant, and compliant with HTTP/1.1 standards.\nFeatures HTTP/1.1 Compliance: Supports basic HTTP request methods (GET, POST, DELETE). Virtual Hosts: Multiple server configurations on the same IP and port, distinguished by the hostname. CGI Support: Handles CGI scripts for dynamic content generation (e.g., PHP, Python). Static File Serving: Serves static files from specified directories. Error Pages: Custom error pages for 404, 403, 500 errors, etc. Chunked Transfer Encoding: Properly handles chunked data in requests. Multithreaded/Multiprocess: Can handle concurrent connections using epoll (or other async mechanisms). Logging: Provides basic logs for connections and errors. Stress-Tested: Battle-tested with tools like siege for stability and performance. For more detailed information click here\n","permalink":"https://almat101.github.io/projects/webserver/webserver/","summary":"\u003ch3 id=\"-subject\"\u003eðŸ”— \u003ca href=\"https://github.com/mlongo03/WebServ_42/blob/main/var/www/WebServer.pdf\"\u003eSubject\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eIn this project, done togheter with my peer Manuele Longo we developed a fully functional HTTP web server built in C++98, supporting essential web server features like GET, POST, and DELETE requests, CGI handling, and more.\u003c/p\u003e\n\u003ch2 id=\"purpose\"\u003ePurpose\u003c/h2\u003e\n\u003cp\u003eThe purpose of this project is to create a robust and efficient web server that can handle various HTTP requests, serve static and dynamic content, and manage multiple virtual hosts. The server is designed to be lightweight, performant, and compliant with HTTP/1.1 standards.\u003c/p\u003e","title":"Webserver"},{"content":"ðŸ”— Subject Inception This project aims to broaden your knowledge of system administration by using Docker. You will virtualize several Docker images, creating them in your new personal virtual machine.\nProject Overview Inception involves setting up and configuring multiple services using Docker containers, each serving a specific purpose. The project uses Docker Compose to manage the containers, networks, and volumes required.\nContainers Nginx Description: Acts as a reverse proxy server to handle HTTP and HTTPS requests. Configuration: Configured to use a self-signed SSL certificate. Ports: Exposes port 443 for HTTPS. Dependencies: Depends on the WordPress container. Build Context: requirements/nginx MariaDB Description: A relational database management system to store WordPress data. Configuration: Uses a custom configuration file. Ports: Exposes port 3306 for database connections. Build Context: requirements/mariadb WordPress Description: A content management system to create and manage websites. Configuration: Automatically configured using a script. Ports: Exposes port 9000 for PHP-FPM. Dependencies: Depends on the MariaDB and Redis containers. Build Context: requirements/wordpress Redis Description: An in-memory data structure store used as a cache for WordPress. Configuration: Uses a custom configuration file. Ports: Exposes port 6379 for Redis connections. Build Context: requirements/bonus/redis FTP Description: An FTP server to manage file uploads. Configuration: Uses a custom configuration file and script. Ports: Exposes ports 21 and 20 for FTP, and a range of ports for passive mode. Dependencies: Depends on the WordPress container. Build Context: requirements/bonus/ftp Hugo Description: A static site generator to create a portfolio website. Configuration: Uses a custom configuration file. Ports: Exposes port 1313 for the Hugo server. Build Context: requirements/bonus/hugo Adminer Description: A lightweight database management tool. Configuration: Accessed via the Nginx container for security. Ports: Exposes port 9001 for internal access. Build Context: requirements/bonus/adminer Prometheus Description: A monitoring tool for collecting and analyzing metrics from services and containers. Configuration: Uses a custom configuration file to monitor specific metrics. Ports: Exposes port 9090 for accessing the Prometheus web interface. Build Context: requirements/bonus/prometheus Node Exporter Description: An exporter for Prometheus to collect hardware and OS metrics from the host machine. Ports: Exposes port 9100 for internal communication with Prometheus. Build Context: requirements/bonus/node-exporter Grafana Description: A visualization tool to create dashboards using metrics collected by Prometheus. Ports: Exposes port 3000 for accessing the Grafana web interface. Build Context: requirements/bonus/grafana Docker Compose Configuration The project uses Docker Compose to manage the containers. The configuration file is located at srcs/docker-compose.yml. It defines the services, networks, and volumes required for the project.\nNetworks inception: A bridge network that connects all the containers. Volumes wordpress: Mounted at /var/www/html for WordPress data. mariadb: Mounted at /var/lib/mysql for MariaDB data. adminer: Mounted at /var/www/adminer for Adminer files. Environment Variables The project requires a .env file with the following environment variables:\nUSER DOMAIN_NAME CERTS KEYS MARIA_DB_NAME MARIA_USER MARIA_PASSWORD MARIA_ROOT_PASSWORD WP_TITLE WP_USER WP_PASSWORD WP_EMAIL WP_ROOT_USER WP_ROOT_PASSWORD WP_ROOT_EMAIL FTP_USER FTP_PASSWORD How to Run the Project Clone the repository and navigate to the project directory.\nCreate a .env file with the required environment variables and put it in srcs folder.\nBuild and start the containers using Makefile:\nmake Access the services via the following URLs:\nWordPress : https://DOMAIN_NAME Hugo : https://DOMAIN_NAME/portfolio Adminer : https://DOMAIN_NAME/adminer Grafana : https://DOMAIN_NAME/grafana Adminer Configuration To connect Adminer to your MariaDB database, use the following settings:\nSystem: MySQL Server:MARIA_DB_NAME Username: MARIA_USER Password: MARIA_PASSWORD Database: MARIA_DB_NAME All values for Server, Username, Password, and Database are sourced from the environment variables defined in your .env file.\nGrafana Configuration When you first access Grafana, you will be prompted to log in:\nLogin:\nUsername: admin Password: admin (You can change this after logging in for the first time) Add a Data Source:\nClick on \u0026ldquo;Add your first data source\u0026rdquo; or navigate to \u0026ldquo;Data Sources\u0026rdquo; from the side menu. Select \u0026ldquo;Prometheus\u0026rdquo; as the data source type. In the Connection settings, set the URL to: http://prometheus:9090 Click on \u0026ldquo;Save \u0026amp; Test\u0026rdquo; to verify the connection. Import a Dashboard:\nGo back to the Grafana home page and click on \u0026ldquo;Create your first dashboard\u0026rdquo;. Click on \u0026ldquo;Find and Import Dashboard\u0026rdquo;. Enter the dashboard ID 1860 (Node Exporter Full) and click \u0026ldquo;Load\u0026rdquo;. In the next step, select Prometheus as the data source and click \u0026ldquo;Import\u0026rdquo;. View the Dashboard:\nYou now have a pre-configured dashboard that displays host-level metrics such as CPU usage, memory utilization, disk usage, and more. The imported dashboard provides detailed insights into the performance of your host machine using metrics collected by the Node Exporter and visualized by Grafana.\nLocal Testing If you want to run the project locally on your PC, set DOMAIN_NAME=localhost in your .env file.\nNote: If you use a different domain name, you will need to update your local /etc/hosts file to map the domain to 127.0.0.1. This file requires sudo permissions to edit. For example:\n127.0.0.1 your-domain-name ","permalink":"https://almat101.github.io/projects/inception/inception/","summary":"\u003ch3 id=\"-subject\"\u003eðŸ”— \u003ca href=\"\"\u003eSubject\u003c/a\u003e\u003c/h3\u003e\n\u003ch1 id=\"inception\"\u003eInception\u003c/h1\u003e\n\u003cp\u003eThis project aims to broaden your knowledge of system administration by using Docker. You will virtualize several Docker images, creating them in your new personal virtual machine.\u003c/p\u003e\n\u003ch2 id=\"project-overview\"\u003eProject Overview\u003c/h2\u003e\n\u003cp\u003eInception involves setting up and configuring multiple services using Docker containers, each serving a specific purpose. The project uses Docker Compose to manage the containers, networks, and volumes required.\u003c/p\u003e\n\u003ch2 id=\"containers\"\u003eContainers\u003c/h2\u003e\n\u003ch3 id=\"nginx\"\u003eNginx\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eDescription\u003c/strong\u003e: Acts as a reverse proxy server to handle HTTP and HTTPS requests.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eConfiguration\u003c/strong\u003e: Configured to use a self-signed SSL certificate.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePorts\u003c/strong\u003e: Exposes port 443 for HTTPS.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDependencies\u003c/strong\u003e: Depends on the WordPress container.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild Context\u003c/strong\u003e: \u003ccode\u003erequirements/nginx\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"mariadb\"\u003eMariaDB\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eDescription\u003c/strong\u003e: A relational database management system to store WordPress data.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eConfiguration\u003c/strong\u003e: Uses a custom configuration file.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePorts\u003c/strong\u003e: Exposes port 3306 for database connections.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild Context\u003c/strong\u003e: \u003ccode\u003erequirements/mariadb\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"wordpress\"\u003eWordPress\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eDescription\u003c/strong\u003e: A content management system to create and manage websites.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eConfiguration\u003c/strong\u003e: Automatically configured using a script.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePorts\u003c/strong\u003e: Exposes port 9000 for PHP-FPM.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDependencies\u003c/strong\u003e: Depends on the MariaDB and Redis containers.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild Context\u003c/strong\u003e: \u003ccode\u003erequirements/wordpress\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"redis\"\u003eRedis\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eDescription\u003c/strong\u003e: An in-memory data structure store used as a cache for WordPress.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eConfiguration\u003c/strong\u003e: Uses a custom configuration file.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePorts\u003c/strong\u003e: Exposes port 6379 for Redis connections.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild Context\u003c/strong\u003e: \u003ccode\u003erequirements/bonus/redis\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"ftp\"\u003eFTP\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eDescription\u003c/strong\u003e: An FTP server to manage file uploads.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eConfiguration\u003c/strong\u003e: Uses a custom configuration file and script.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePorts\u003c/strong\u003e: Exposes ports 21 and 20 for FTP, and a range of ports for passive mode.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDependencies\u003c/strong\u003e: Depends on the WordPress container.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild Context\u003c/strong\u003e: \u003ccode\u003erequirements/bonus/ftp\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"hugo\"\u003eHugo\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eDescription\u003c/strong\u003e: A static site generator to create a portfolio website.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eConfiguration\u003c/strong\u003e: Uses a custom configuration file.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePorts\u003c/strong\u003e: Exposes port 1313 for the Hugo server.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild Context\u003c/strong\u003e: \u003ccode\u003erequirements/bonus/hugo\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"adminer\"\u003eAdminer\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eDescription\u003c/strong\u003e: A lightweight database management tool.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eConfiguration\u003c/strong\u003e: Accessed via the Nginx container for security.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePorts\u003c/strong\u003e: Exposes port 9001 for internal access.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild Context\u003c/strong\u003e: requirements/bonus/adminer\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"prometheus\"\u003ePrometheus\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eDescription\u003c/strong\u003e: A monitoring tool for collecting and analyzing metrics from services and containers.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eConfiguration\u003c/strong\u003e: Uses a custom configuration file to monitor specific metrics.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePorts\u003c/strong\u003e: Exposes port 9090 for accessing the Prometheus web interface.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild Context\u003c/strong\u003e: requirements/bonus/prometheus\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"node-exporter\"\u003eNode Exporter\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eDescription\u003c/strong\u003e: An exporter for Prometheus to collect hardware and OS metrics from the host machine.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePorts\u003c/strong\u003e: Exposes port 9100 for internal communication with Prometheus.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild Context\u003c/strong\u003e: requirements/bonus/node-exporter\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"grafana\"\u003eGrafana\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eDescription\u003c/strong\u003e: A visualization tool to create dashboards using metrics collected by Prometheus.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePorts\u003c/strong\u003e: Exposes port 3000 for accessing the Grafana web interface.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBuild Context\u003c/strong\u003e: requirements/bonus/grafana\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"docker-compose-configuration\"\u003eDocker Compose Configuration\u003c/h2\u003e\n\u003cp\u003eThe project uses Docker Compose to manage the containers. The configuration file is located at \u003ccode\u003esrcs/docker-compose.yml\u003c/code\u003e. It defines the services, networks, and volumes required for the project.\u003c/p\u003e","title":"Inception"},{"content":"What is Docker? Docker is an open-source platform that automates the deployment, scaling, and management of applications using containerization. Containers allow developers to package an application with all its dependencies into a standardized unit for software development.\nKey Concepts Containers Containers are lightweight, portable, and self-sufficient units that include everything needed to run a piece of software, including the code, runtime, libraries, and system tools. They are isolated from each other and the host system, ensuring consistent behavior across different environments.\nImages Docker images are read-only templates used to create containers. An image includes the application code, libraries, dependencies, and other files needed to run the application. Images are built from a Dockerfile, which contains a series of instructions for creating the image.\nDockerfile A Dockerfile is a text file that contains a series of instructions for building a Docker image. It specifies the base image, application code, dependencies, and other configurations needed to create the image.\nDocker Hub Docker Hub is a cloud-based registry service that allows you to store, share, and manage Docker images. It provides a centralized location for finding and distributing container images.\nDocker Compose Docker Compose is a tool for defining and running multi-container Docker applications. It uses a YAML file to configure the application\u0026rsquo;s services, networks, and volumes, allowing you to manage complex applications with ease.\nBenefits of Docker Portability Docker containers can run on any system that supports Docker, ensuring consistent behavior across different environments, such as development, testing, and production.\nIsolation Containers provide process and filesystem isolation, ensuring that applications run in their own environments without interfering with each other or the host system.\nScalability Docker makes it easy to scale applications horizontally by adding or removing containers as needed. This allows for efficient resource utilization and improved application performance.\nEfficiency Containers are lightweight and share the host system\u0026rsquo;s kernel, making them more efficient than traditional virtual machines. This results in faster startup times and reduced resource consumption.\nVersion Control Docker images are versioned, allowing you to track changes and roll back to previous versions if needed. This makes it easy to manage application updates and ensure consistency across different environments.\nHow Docker Works Build: Create a Dockerfile with the necessary instructions to build the image. Use the docker build command to create the image from the Dockerfile. Ship: Push the image to Docker Hub or another registry using the docker push command. This makes the image available for others to use. Run: Use the docker run command to create and start a container from the image. The container runs the application in an isolated environment. Example Dockerfile Here is an example of a simple Dockerfile for an ubuntu-based application:\n# Use the official Ubuntu image as the base image FROM ubuntu:latest # Update and upgrade the package list RUN apt-get update \u0026amp;\u0026amp; apt-get upgrade -y # Install necessary packages RUN apt-get install -y vim # Start the application CMD [\u0026#34;bash\u0026#34;] In this case we install only vim, but you can install and configure every package you want, like nginx, wordpress, apache, mariadb etc\n","permalink":"https://almat101.github.io/blog/docker/docker/","summary":"\u003ch2 id=\"what-is-docker\"\u003eWhat is Docker?\u003c/h2\u003e\n\u003cp\u003eDocker is an open-source platform that automates the deployment, scaling, and management of applications using containerization. Containers allow developers to package an application with all its dependencies into a standardized unit for software development.\u003c/p\u003e\n\u003ch2 id=\"key-concepts\"\u003eKey Concepts\u003c/h2\u003e\n\u003ch3 id=\"containers\"\u003eContainers\u003c/h3\u003e\n\u003cp\u003eContainers are lightweight, portable, and self-sufficient units that include everything needed to run a piece of software, including the code, runtime, libraries, and system tools. They are isolated from each other and the host system, ensuring consistent behavior across different environments.\u003c/p\u003e","title":"Docker"},{"content":"Setting Up the VM After installing a new VM with a Debian image, follow these steps to set up your environment for the Inception project. This guide includes installing Docker, Docker Compose, Git, and Vim.\nStep-by-Step Guide 1. Update and Upgrade the System First, update and upgrade the package list to ensure you have the latest packages.\nsudo apt-get update \u0026amp;\u0026amp; sudo apt-get upgrade -y 2. Install Git and Vim Then, install Git for downloading inception and vim modify files\nsudo apt install -y git vim 3. Install Docker Set up docker\u0026rsquo;s apt repository: # Add Docker\u0026#39;s official GPG key: sudo apt-get update sudo apt-get install ca-certificates curl sudo install -m 0755 -d /etc/apt/keyrings sudo curl -fsSL https://download.docker.com/linux/debian/gpg -o /etc/apt/keyrings/docker.asc sudo chmod a+r /etc/apt/keyrings/docker.asc # Add the repository to Apt sources: echo \\ \u0026#34;deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/debian \\ $(. /etc/os-release \u0026amp;\u0026amp; echo \u0026#34;$VERSION_CODENAME\u0026#34;) stable\u0026#34; | \\ sudo tee /etc/apt/sources.list.d/docker.list \u0026gt; /dev/null sudo apt-get update Install docker packages sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin Verify the installation by running the hello-world image: sudo docker run hello-world ","permalink":"https://almat101.github.io/blog/vm/vm/","summary":"\u003ch1 id=\"setting-up-the-vm\"\u003eSetting Up the VM\u003c/h1\u003e\n\u003cp\u003eAfter installing a new VM with a Debian image, follow these steps to set up your environment for the Inception project. This guide includes installing Docker, Docker Compose, Git, and Vim.\u003c/p\u003e\n\u003ch2 id=\"step-by-step-guide\"\u003eStep-by-Step Guide\u003c/h2\u003e\n\u003ch3 id=\"1-update-and-upgrade-the-system\"\u003e1. Update and Upgrade the System\u003c/h3\u003e\n\u003cp\u003eFirst, update and upgrade the package list to ensure you have the latest packages.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003esudo apt-get update \u003cspan style=\"color:#f92672\"\u003e\u0026amp;\u0026amp;\u003c/span\u003e sudo apt-get upgrade -y\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"2-install-git-and-vim\"\u003e2. Install Git and Vim\u003c/h3\u003e\n\u003cp\u003eThen, install Git for downloading inception and vim modify files\u003c/p\u003e","title":"Virtual Machine"}]